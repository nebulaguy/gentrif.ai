{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a12774",
   "metadata": {},
   "source": [
    "## Gentrification Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7622202",
   "metadata": {},
   "source": [
    "classify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25db5c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6v/5g0thsxx6f95p3lg__t_4hxc0000gn/T/ipykernel_60065/2980900011.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"../data/Final_Dataset/final_merged_dataset.csv\")\n",
    "\n",
    "# Calculate thresholds based on percentiles\n",
    "thresholds = {\n",
    "    \"ZHVF 1-Year Forecast (%)\": data[\"ZHVF 1-Year Forecast (%)\"].quantile(0.50),\n",
    "    \"Percent Change\": data[\"Percent Change\"].quantile(0.50),\n",
    "    \"Mean Income\": data[\"Mean Income\"].quantile(0.50),\n",
    "    \"luxury_business_count\": data[\"luxury_business_count\"].quantile(0.50),\n",
    "    \"SizeRank_lower\": data[\"SizeRank\"].quantile(0.40),\n",
    "    \"SizeRank_upper\": data[\"SizeRank\"].quantile(0.60)\n",
    "}\n",
    "\n",
    "# Function to count criteria met for gentrification\n",
    "def count_criteria_met(row):\n",
    "    criteria_count = 0\n",
    "    criteria_count += (row[\"ZHVF 1-Year Forecast (%)\"] > thresholds[\"ZHVF 1-Year Forecast (%)\"])\n",
    "    criteria_count += (row[\"Percent Change\"] > thresholds[\"Percent Change\"])\n",
    "    criteria_count += (row[\"Mean Income\"] > thresholds[\"Mean Income\"])\n",
    "    criteria_count += (row[\"luxury_business_count\"] > thresholds[\"luxury_business_count\"])\n",
    "    criteria_count += (thresholds[\"SizeRank_lower\"] < row[\"SizeRank\"] < thresholds[\"SizeRank_upper\"])\n",
    "    return criteria_count\n",
    "\n",
    "# Create the gentrified column based on criteria\n",
    "data[\"gentrified\"] = data.apply(lambda row: 1 if count_criteria_met(row) >= 3 else 0, axis=1)\n",
    "\n",
    "# Save the modified dataset\n",
    "data.to_csv(\"modified_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4248606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.0-1-cp310-cp310-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.26.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.4.0-1-cp310-cp310-macosx_12_0_arm64.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Downloading scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958f6ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Extract features and target\n",
    "features = [\"SizeRank\", \"ZHVF 1-Year Forecast (%)\", \"Percent Change\", \"Mean Income\", \"luxury_business_count\"]\n",
    "X = data[features]\n",
    "y = data[\"gentrified\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the model and scaler\n",
    "joblib.dump(clf, 'gentrification_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f11a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Create a dataframe with test set ZIP codes, actual values, and predictions\n",
    "results = pd.DataFrame({\n",
    "    'ZIP Code': X_test.index,\n",
    "    'Actual Value': y_test,\n",
    "    'Predicted Value': y_pred\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV\n",
    "results.to_csv('model_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2cbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset\n",
    "new_data = pd.read_csv(\"../data/Final_Dataset/final_merged_dataset.csv\")\n",
    "\n",
    "# Ensure it has the required features\n",
    "required_features = [\"SizeRank\", \"ZHVF 1-Year Forecast (%)\", \"Percent Change\", \"Mean Income\", \"luxury_business_count\"]\n",
    "X_new = new_data[required_features]\n",
    "\n",
    "# Use the previously defined scaler to standardize these features\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Use the trained model to make predictions\n",
    "y_new_pred = clf.predict(X_new_scaled)\n",
    "\n",
    "# Add the predictions to the new dataset\n",
    "new_data[\"predicted_gentrified\"] = y_new_pred\n",
    "\n",
    "# Optionally, save the new dataset with predictions to a CSV\n",
    "new_data.to_csv(\"new_dataset_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf3926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n"
     ]
    }
   ],
   "source": [
    "predicted_data = pd.read_csv('new_dataset_with_predictions.csv')\n",
    "gentrified = predicted_data[\"predicted_gentrified\"].value_counts().get(1,0)\n",
    "print(gentrified)\n",
    "new_data_only_gentrification = new_data[new_data[\"predicted_gentrified\"] == 1]\n",
    "new_data_only_gentrification.to_csv(\"new_dataset_only_gentrification\")\n",
    "\n",
    "new_data_no_gentrification = new_data[new_data[\"predicted_gentrified\"] == 0]\n",
    "new_data_no_gentrification.to_csv(\"new_dataset_no_gentrification\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f6c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db_file = \"data.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "table_name = \"table1\"\n",
    "new_data.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50609eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.64%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
